{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd33b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.utils import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5c810",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network and LSTM for IMDB Sentiment Analysis\n",
    "\n",
    "Apparently this is a pre-requisite for understanding Transformers, so let's do it\n",
    "\n",
    "In this one, I will implement the Recurrent Neural Network. First with the high level functional api call, then writing the layer as a custom subclass\n",
    "\n",
    "The mlp had 2.5 million parameters and .88 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60402cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced these hyper parameters since this is so much slower\n",
    "EMBED_DIM = 100\n",
    "VOCABULARY_SIZE = 8000\n",
    "MAX_SEQUENCE_LEN = 64\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words = VOCABULARY_SIZE,\n",
    "    skip_top = 0,\n",
    "    max_len=MAX_SEQUENCE_LEN,\n",
    "    seed=113,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3,   \n",
    ")\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LEN)\n",
    "word_index = keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58f22a",
   "metadata": {},
   "source": [
    "## Implementation with keras.layers.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518d3af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"imdb_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"imdb_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">800,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m800,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m42,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">842,305</span> (3.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m842,305\u001b[0m (3.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">842,305</span> (3.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m842,305\u001b[0m (3.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.6172 - loss: 0.5980 - val_accuracy: 0.8096 - val_loss: 0.4002\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - accuracy: 0.8302 - loss: 0.3726 - val_accuracy: 0.8250 - val_loss: 0.3849\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - accuracy: 0.8619 - loss: 0.3122 - val_accuracy: 0.7970 - val_loss: 0.4078\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.8837 - loss: 0.2722 - val_accuracy: 0.8212 - val_loss: 0.3960\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - accuracy: 0.8985 - loss: 0.2469 - val_accuracy: 0.8162 - val_loss: 0.4184\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8264 - loss: 0.3835\n",
      "Test Loss: 0.3794962465763092\n",
      "Test Accuracy: 0.8289200067520142\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n",
    "x = keras.layers.Embedding(VOCABULARY_SIZE, EMBED_DIM) (inputs)\n",
    "x = keras.layers.LSTM(64)(x) # That's it?\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs = outputs ,name='imdb_model')\n",
    "model.summary()\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=3,      \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss= keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.RMSprop(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping_callback])\n",
    "\n",
    "test_scores=model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Loss: {test_scores[0]}')\n",
    "print(f'Test Accuracy: {test_scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1441ce1",
   "metadata": {},
   "source": [
    "## Implementation with custom layers\n",
    "\n",
    "Just learned now that unlike in the jax docs or my familiar linear algebra textbook, the vectors in Keras are conventionally row vectors, which works out more nicely when batch is the first dimension of input. As a result, matrix multiplications look like Vector @ Matrix (of shape input, output)\n",
    "\n",
    "Also, a quick thing I noticed. Applying dimensionality analysis shows that b has to be broadcasted in order to work with these functions. b is written as a vector, but needs to be illegally added to a batched matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450b33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units=units\n",
    "        self.activation = keras.activations.tanh\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape is (batch_size, timesteps, input_features)\n",
    "        input_features = input_shape[-1]\n",
    "        self.wx = self.add_weight(\n",
    "            shape=(input_features, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        self.wh = self.add_weight(\n",
    "            shape=(self.units,self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size = ops.shape(inputs)[0]\n",
    "        h = ops.zeros(shape=(batch_size, self.units))\n",
    "        for x_t in ops.unstack(inputs, axis=1): \n",
    "            z = ops.matmul(x_t, self.wx) + ops.matmul(h, self.wh) + self.b\n",
    "            h = self.activation(z)\n",
    "        return h\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8345af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_features = input_shape[-1]\n",
    "        self.sw1 = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.sw2 = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.sw3 = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.sw4 = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.xw1 = self.add_weight(\n",
    "            shape=(input_features, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.xw2 = self.add_weight(\n",
    "            shape=(input_features, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.xw3 = self.add_weight(\n",
    "            shape=(input_features, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.xw4 = self.add_weight(\n",
    "            shape=(input_features, self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b1 = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        self.b3 = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        self.b4 = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = ops.shape(inputs)[0]\n",
    "        #short term and long term memory vectors\n",
    "        s = ops.zeros(shape=(batch_size, self.units))\n",
    "        l = ops.zeros(shape=(batch_size, self.units))\n",
    "        for x_t in ops.unstack(inputs, axis=1):\n",
    "            # forget gate. sigmoid\n",
    "            l *= keras.activations.sigmoid(ops.matmul(x_t,self.xw1) + ops.matmul(s,self.sw1) + self.b1)\n",
    "            # input gate. tanh and sigmoid\n",
    "            z1 = keras.activations.sigmoid(ops.matmul(x_t,self.xw2) + ops.matmul(s,self.sw2) + self.b2)\n",
    "            z2 = keras.activations.tanh(ops.matmul(x_t,self.xw3) + ops.matmul(s,self.sw3) + self.b3)\n",
    "            l += z1 * z2\n",
    "            # output gate. sigmoid and tanh\n",
    "            z1 = keras.activations.sigmoid(ops.matmul(x_t,self.xw4) + ops.matmul(s,self.sw4) + self.b4)\n",
    "            z2 = keras.activations.tanh(l)\n",
    "            s = z1 * z2\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88607ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"imdb_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"imdb_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">800,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MyLSTM</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m800,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_lstm_3 (\u001b[38;5;33mMyLSTM\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m42,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">842,305</span> (3.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m842,305\u001b[0m (3.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">842,305</span> (3.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m842,305\u001b[0m (3.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 30ms/step - accuracy: 0.5408 - loss: 0.6596 - val_accuracy: 0.7948 - val_loss: 0.4762\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.8159 - loss: 0.4045 - val_accuracy: 0.8138 - val_loss: 0.4239\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8574 - loss: 0.3375 - val_accuracy: 0.8200 - val_loss: 0.3939\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8650 - loss: 0.3103 - val_accuracy: 0.8240 - val_loss: 0.3965\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.8796 - loss: 0.2900 - val_accuracy: 0.8116 - val_loss: 0.3931\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8095 - loss: 0.3935\n",
      "Test Loss: 0.387563019990921\n",
      "Test Accuracy: 0.8129199743270874\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n",
    "x = keras.layers.Embedding(VOCABULARY_SIZE, EMBED_DIM) (inputs)\n",
    "# x = MyRNN(64)(x) # That's it?\n",
    "x = MyLSTM(64)(x)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs = outputs ,name='imdb_model')\n",
    "model.summary()\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=3,      \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss= keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.RMSprop(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping_callback])\n",
    "\n",
    "test_scores=model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Loss: {test_scores[0]}')\n",
    "print(f'Test Accuracy: {test_scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afe4c2",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "I'm pretty proud of that LSTM implementation. Got it in one try by carefully tracing the matrix dimensions. Notably, I could improve it by running more computations at once. For instance, many of the gates can be computed at the same time using one larger matrix. Additionally, the keras.layers.RNN class allows me to instantiate with a custom Cell containing this new call function. Appararently that would migrate the implementation of the for loop from the python interpretor to the low level backend and make it a lot faster. \n",
    "\n",
    "Empirically though, I do not notice a difference in speed (I don't have the compatible hardware for gpu acceleration), so I don't expect that refactoring exercise to be that satisfying. Additionally, I think I got the pedadogical takeaways for this module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
