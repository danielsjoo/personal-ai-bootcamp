{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd354277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "from jax import random, jit, vmap, value_and_grad\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879fb9c",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis in jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4f83c",
   "metadata": {},
   "source": [
    "## The Dense Neural Network: A naive approach\n",
    "\n",
    "Currently just realized that building the tokenizer from scratch is a very complicated problem that I will try doing later. For now, we will take the dataset as is. The IMDB reviews are broken up and encoded as an array of numbers [0,2,3] for instance. Each value in the array corresponds to a different word token that can easily be rebuilt by the encoder in the tfds given ds_info. This encoding is very special; a different problem (that has already been solved) is on how to make this encoding efficient and meaningful (so for instance, this is not a given simple word to number assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4239fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "def preprocess_data():\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "        'imdb_reviews/subwords8k',\n",
    "        split = ['train', 'test'],\n",
    "        with_info=True,\n",
    "        as_supervised= True\n",
    "    )\n",
    "    \n",
    "    # for debugging, allows us to read encoded reviews\n",
    "    tokenizer = ds_info.features['text'].encoder\n",
    "    # shape of each data input. first one is the review input of length (max_len) and the second is for the label (scalar)\n",
    "    def truncate_text(text, label):\n",
    "        return text[:MAX_LEN], label\n",
    "    # Apply this function to every example in the datasets\n",
    "    ds_train = ds_train.map(truncate_text)\n",
    "    ds_test = ds_test.map(truncate_text)\n",
    "    padded_shapes = ([MAX_LEN], [])\n",
    "    # 1000 is buffer size. It pulls first 64 from shuffled buffer\n",
    "    ds_train = ds_train.shuffle(1000).padded_batch(BATCH_SIZE, padded_shapes=padded_shapes)\n",
    "    # test data is batched because that is more scalable than my previous approach for MNIST\n",
    "    ds_test = ds_test.padded_batch(BATCH_SIZE, padded_shapes=padded_shapes)\n",
    "    return ds_train, ds_test, tokenizer\n",
    "\n",
    "layer_sizes = [MAX_LEN, 128, 128, 1]\n",
    "\n",
    "def init_mlp_params(layer_sizes, key):\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_sizes) - 1)\n",
    "    for nin, nout, layer_key in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
    "        w_key, b_key = random.split(layer_key)\n",
    "        layer_params = {\n",
    "            'w': random.normal(w_key, (nout, nin)),\n",
    "            'b': jnp.zeros((nout,))\n",
    "        }\n",
    "        params.append(layer_params)\n",
    "    return params\n",
    "\n",
    "def mlp_apply(params, inputs):\n",
    "    x = inputs\n",
    "    for layer_params in params[:-1]:\n",
    "        z = layer_params['w'] @ x + layer_params['b']\n",
    "        x = jax.nn.relu(z)\n",
    "    final_layer_params = params[-1]\n",
    "    output = final_layer_params['w'] @ x + final_layer_params['b']\n",
    "    output = jax.nn.sigmoid(output)\n",
    "    return output\n",
    "    \n",
    "def loss_fn(params, inputs, targets):\n",
    "    predictions = vmap(mlp_apply, in_axes=(None,0))(params, inputs)\n",
    "    return jnp.mean((jnp.squeeze(predictions)-targets) ** 2)\n",
    "\n",
    "weight_decay = 0.0001\n",
    "solver = optax.adamw(learning_rate = 0.001, weight_decay=weight_decay)\n",
    "@jit\n",
    "def train_step(params, inputs, targets, opt_state):\n",
    "    loss, grads = value_and_grad(loss_fn)(params, inputs, targets)\n",
    "    updates, opt_state = solver.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return loss, new_params, opt_state\n",
    "\n",
    "def test_accuracy(params, ds_test):\n",
    "    batch_accuracies = []\n",
    "    for batch in tfds.as_numpy(ds_test):\n",
    "        inputs, labels = batch\n",
    "        batch_predictions = vmap(mlp_apply, in_axes=(None,0))(params,inputs)\n",
    "        batch_accuracies.append(jnp.mean(batch_predictions.round() == labels))\n",
    "    return sum(batch_accuracies) / len(batch_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90508dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "ds_train, ds_test, tokenizer = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df446e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "params = init_mlp_params(layer_sizes, key)\n",
    "opt_state = solver.init(params)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# total_training_start_time = time.time()\n",
    "# for epoch in range(epochs):\n",
    "#     for test_batch in tfds.as_numpy(ds_train):\n",
    "#         reviews, labels = test_batch\n",
    "#         loss, params, opt_state = train_step(params, reviews, labels, opt_state)\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'accuracy at epoch {epoch}: {test_accuracy(params, ds_test)}')\n",
    "# print(f'accuracy at epoch 50: {test_accuracy(params, ds_test)}')\n",
    "# total_training_duration = time.time() - total_training_start_time\n",
    "# print(f'Training finished in {total_training_duration:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea7d45",
   "metadata": {},
   "source": [
    "## The Dense Neural Network with upgrades\n",
    "1. Add an embedding layer. This translates meaningless token values (50 for 'movie' for instance) to a mapped vector of size EMBED_DIM. \n",
    "   1. Instead of inputting a (MAX_LEN * EMBED_DIM) size vector after flattening this input, we summarize with global average pooling: we take the mean of the (MAX_LEN, EMBED_DIM) vector into a (EMBED_DIM) vector\n",
    "   2. This input does not maintain the order of the words, simple meanings associated with each word\n",
    "   3. Include Embed layer in params, and also before MLP in forward pass\n",
    "2. Incorporate Binary Cross Entropy for Loss\n",
    "   1. Sigmoid converts values into probalities between 0 and 1\n",
    "   2. Previous attempt uses p or 1-p squared to calculate loss\n",
    "   3. BCE uses -log(1-p) to calculuate loss. This non 0-1 bounded output makes penalties bigger near the tails (0 and 1)\n",
    "   4. We still keep just the sigmoid for accuracy\n",
    "   5. notably, we apply sigmoid and bce at the same time (sigmoid_binary_cross_entropy) because library designers made this more robust for floating point arithmetic\n",
    "      1. Means we predict the logits, and apply sigmoid in accuracy, sigmoid_binary_cross_entropy in loss\n",
    "3. Tune Hyperparameters\n",
    "   1. Will try [200, 256, 128, 1]\n",
    "4. Implement Dropout for overfitting\n",
    "5. Integrate Pre-trained GloVe embeddings\n",
    "   1. Will be important to translate this to the tokenization system that subwords 8k already did and handle out of vocabulary words\n",
    "\n",
    "we keep the same preprocess data function\n",
    "\n",
    "Due to the complexity, I think now is a good time to switch to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce8306",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 200\n",
    "VOCAB_SIZE = 10000\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "mlp_layer_sizes = [EMBED_DIM, 256, 128, 1]\n",
    "\n",
    "def init_mlp_params(layer_sizes, key):\n",
    "    mlp_params = []\n",
    "    embed_key, mlp_key = random.split(key)\n",
    "    embedding_matrix = random.normal(embed_key, (VOCAB_SIZE, EMBED_DIM))\n",
    "    mlp_keys = random.split(mlp_key, len(layer_sizes) - 1)\n",
    "    for nin, nout, layer_key in zip(layer_sizes[:-1], layer_sizes[1:], mlp_keys):\n",
    "        w_key, b_key = random.split(layer_key)\n",
    "        layer_params = {\n",
    "            'w': random.normal(w_key, (nout, nin)),\n",
    "            'b': jnp.zeros((nout,))\n",
    "        }\n",
    "        mlp_params.append(layer_params)\n",
    "    return {\n",
    "        'embedding': embedding_matrix,\n",
    "        'mlp': mlp_params\n",
    "    }\n",
    "\n",
    "def mlp_apply(params, inputs):\n",
    "    x = params['embedding'][inputs]\n",
    "    x = jnp.mean(x, axis=0)\n",
    "    for layer_params in params['mlp'][:-1]:\n",
    "        z = layer_params['w'] @ x + layer_params['b']\n",
    "        x = jax.nn.relu(z)\n",
    "    final_layer_params = params['mlp'][-1]\n",
    "    output = final_layer_params['w'] @ x + final_layer_params['b']\n",
    "    return output\n",
    "\n",
    "def loss_fn(params, inputs, targets):\n",
    "    prediction_logits = vmap(mlp_apply, in_axes=(None,0))(params, inputs)\n",
    "    prediction_logits = jnp.squeeze(prediction_logits)\n",
    "    return jnp.mean(optax.sigmoid_binary_cross_entropy(prediction_logits, targets))\n",
    "\n",
    "weight_decay = 0.0001\n",
    "solver = optax.adamw(learning_rate = 0.001, weight_decay=weight_decay)\n",
    "@jit\n",
    "def train_step(params, inputs, targets, opt_state):\n",
    "    loss, grads = value_and_grad(loss_fn)(params, inputs, targets)\n",
    "    updates, opt_state = solver.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return loss, new_params, opt_state\n",
    "\n",
    "def test_accuracy(params, ds_test):\n",
    "    batch_accuracies = []\n",
    "    for batch in tfds.as_numpy(ds_test):\n",
    "        inputs, labels = batch\n",
    "        logits = vmap(mlp_apply, in_axes=(None,0))(params,inputs)\n",
    "        probabilities = jax.nn.sigmoid(logits)\n",
    "        predictions = jnp.squeeze(probabilities).round()\n",
    "        batch_accuracies.append(jnp.mean(predictions == labels))\n",
    "    return jnp.mean(jnp.array(batch_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ce8c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0: Avg Loss = 36.8940, Accuracy = 0.5988\n",
      "After epoch 10: Avg Loss = 0.5627, Accuracy = 0.7163\n",
      "After epoch 20: Avg Loss = 0.1415, Accuracy = 0.7389\n",
      "After epoch 30: Avg Loss = 0.0996, Accuracy = 0.7555\n",
      "After epoch 40: Avg Loss = 0.0592, Accuracy = 0.7646\n",
      "accuracy at epoch 50: 0.7655929923057556\n",
      "Training finished in 116.62s\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(42)\n",
    "params = init_mlp_params(mlp_layer_sizes, key)\n",
    "opt_state = solver.init(params)\n",
    "\n",
    "epochs = 50\n",
    "total_training_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for test_batch in tfds.as_numpy(ds_train):\n",
    "        reviews, labels = test_batch\n",
    "        loss, params, opt_state = train_step(params, reviews, labels, opt_state)\n",
    "        epoch_losses.append(loss)\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = test_accuracy(params, ds_test)\n",
    "        print(f'After epoch {epoch}: Avg Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
    "print(f'accuracy at epoch 50: {test_accuracy(params, ds_test)}')\n",
    "total_training_duration = time.time() - total_training_start_time\n",
    "print(f'Training finished in {total_training_duration:.2f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
