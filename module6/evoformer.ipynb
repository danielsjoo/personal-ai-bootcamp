{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65361203",
   "metadata": {},
   "source": [
    "# The Alphafold Evoformer\n",
    "\n",
    "Oh man this was my goal from the start. And now I think I have a grasp of the architecture well enough to try it myself. This evoformer is from alphafold 2. If I can do this, maybe I'll try my hand at the alphafold 3 pairformer and diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe50bc9",
   "metadata": {},
   "source": [
    "## some stuff about the inputs\n",
    "\n",
    "There is a lot of parsing and preparation in the actual project. But to skip ahead, alphafold 2 is built from 3 parts:\n",
    "\n",
    "MSA Representation\n",
    "The initial sequence is checked against a genetic database search to find similar sequences that lead to similar proteins. Due to the random nature of mutations, these differences between the protein in different species should have random differences, but evolutionary pressure causes all the sequences, which are slightly different, to end with a simlar structure. A strategy of Alphafold 2 is to look at this evolutionary history and analyze residues that may have coevolved and infer the importance of such residue for the actual structure. This raw data of the sequences is called the multiple sequence alignment, or MSA. The MSA representation is a matrix of size (number of sequences, number of residues, embedding channels for each residue). Importantly, if we analyze the matrix row-wise, we are studying the effect of residues on eachother on the same sequence. If we analyze the matrix column wise, we are witnessing the evolutionary history/context of that residue. The independence of these information sources kind of allow us to apply attention to each independently and drastically reduce the attention context size to a reasonable level.\n",
    "\n",
    "Pair Representation\n",
    "A square matrix with size (number of residues) squared serves as a template for the pair representation matrix, where an entry i,j indicates the interaction between two residues. The initialization is the addition of two things:\n",
    "- Concatenated one hot encodings: Each residue is given a one hot encoding and the two vectors in the pair are added for the i,j entry.\n",
    "- Relative positional embeddings: A vector that encodes the separation between residue i and j in the chain. This vector is not a simple integer encoding the difference in residues but rather a a set of sinusoidal functions of different frequencies (like how positional embeddings are done in nlp)\n",
    "The resulting vector length becomes the third matrix dimension that represents the embedding dimension of each residue pair.\n",
    "\n",
    "In the alphafold 2 paper, the length of the embedded information of a residue or residue pair is described as the channel dimension. Personally, I prefer the sound of embedding dimension, but it doesn't matter mathematically. I believe the reason for Deepmind's wordchoice is because of this projects reliance on 2D matrix data as opposed to the nlp 1D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "from typing import Optional\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc5e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMHA(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        head_dim: int,\n",
    "        *,\n",
    "        bias_channels: Optional[int] = None,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.attention_dim = self.num_heads * self.head_dim\n",
    "\n",
    "        self.layer_norm_m = nnx.LayerNorm(num_features=embed_dim, rngs=rngs)\n",
    "        self.qkv_proj = nnx.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=self.attention_dim * 3,\n",
    "            use_bias=False,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.gate_proj = nnx.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=self.attention_dim,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.output_proj = nnx.Linear(\n",
    "            in_features=self.attention_dim,\n",
    "            out_features=embed_dim,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        if bias_channels is not None:\n",
    "            self.layer_norm_z = nnx.LayerNorm(num_features=bias_channels, rngs=rngs)\n",
    "            self.pair_bias_proj = nnx.Linear(\n",
    "                in_features=bias_channels,\n",
    "                out_features=num_heads,\n",
    "                use_bias=False,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "        else:\n",
    "            self.layer_norm_z = None\n",
    "            self.pair_bias_proj = None\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        m: jax.Array,\n",
    "        z: Optional[jax.Array] = None,\n",
    "        mask: Optional[jax.Array] = None\n",
    "    ) -> jax.Array:\n",
    "        seq_len, _ = m.shape\n",
    "\n",
    "        gate_values = self.gate_proj(m)\n",
    "        gate_values = gate_values.reshape(seq_len, self.num_heads, self.head_dim)\n",
    "        # gate: (num_heads, seq_len, head_dim)\n",
    "        gate = nnx.sigmoid(jnp.transpose(gate_values, (1, 0, 2)))\n",
    "\n",
    "        m_norm = self.layer_norm_m(m)\n",
    "        qkv = self.qkv_proj(m_norm)\n",
    "        qkv = qkv.reshape(seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = jnp.transpose(qkv, (1, 2, 0, 3))\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # (num_heads, seq_len, head_dim)\n",
    "\n",
    "        attention_logits = jnp.einsum(\"hid,hjd->hij\", q, k)\n",
    "        attention_logits /= math.sqrt(self.head_dim)\n",
    "\n",
    "        if z is not None: # (seq_len, seq_len, bias_channels)\n",
    "            z_norm = self.layer_norm_z(z)\n",
    "            pair_bias = self.pair_bias_proj(z_norm) # (seq_len, seq_len, num_heads)\n",
    "            pair_bias = jnp.transpose(pair_bias, (2, 0, 1)) # (num_heads, seq_len, seq_len)\n",
    "            attention_logits += pair_bias\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_mask = nnx.make_attention_mask(mask, mask)\n",
    "            attention_logits += jnp.where(attention_mask, 0.0, -1e9)\n",
    "\n",
    "        weights = nnx.softmax(attention_logits, axis=-1)\n",
    "\n",
    "        attn_output = jnp.einsum(\"hij,hjd->hid\", weights, v) #(num_heads, seq_len, head_dim)\n",
    "        gated_attn_output = attn_output * gate\n",
    "\n",
    "        gated_attn_output = jnp.transpose(gated_attn_output, (1, 0, 2)) #(seq_len, num_heads, head_dim)\n",
    "        concatenated_output = gated_attn_output.reshape(seq_len, self.attention_dim) # (seq_len, attention_dim)\n",
    "        final_output = self.output_proj(concatenated_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb06d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_channels = 384\n",
    "msa_num_heads = 8\n",
    "msa_head_dim = 32\n",
    "pair_channels = 128\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "dropout_key = jax.random.PRNGKey(1)\n",
    "rngs = nnx.Rngs(params=key, dropout=dropout_key)\n",
    "\n",
    "# attention over residues\n",
    "class MSARowAttentionWithPairBias(nnx.Module):\n",
    "    def __init__(self, msa_channels, pair_channels, *, num_heads=8, head_dim=32,rngs: nnx.Rngs):\n",
    "        self.gated_mha = GatedMHA(\n",
    "            embed_dim=msa_channels, \n",
    "            num_heads=num_heads, \n",
    "            head_dim=head_dim,\n",
    "            bias_channels=pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.vmapped_mha = nnx.vmap(\n",
    "            self.gated_mha,\n",
    "            in_axes=(0,None),\n",
    "            out_axes=0,\n",
    "        )\n",
    "    \n",
    "    def __call__(self, msa: jax.Array, z: jax.Array) -> jax.Array:\n",
    "        return self.vmapped_mha(msa,z)\n",
    "\n",
    "# attention across sequences\n",
    "class MSAColumnAttention(nnx.Module):\n",
    "    def __init__(self, msa_channels, num_heads=8, head_dim=32, *, rngs: nnx.Rngs):\n",
    "        self.gated_mha = GatedMHA(\n",
    "            embed_dim=msa_channels, \n",
    "            num_heads=num_heads, \n",
    "            head_dim=head_dim,\n",
    "            bias_channels=None,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.vmapped_mha = nnx.vmap(\n",
    "            self.gated_mha,\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )\n",
    "    \n",
    "    def __call__(self, msa: jax.Array) -> jax.Array:\n",
    "        return self.vmapped_mha(msa)\n",
    "\n",
    "# transition\n",
    "class MSATransition(nnx.Module):\n",
    "    def __init__(self, msa_channels, *, rngs: nnx.Rngs, n=4):\n",
    "        self.layer_norm = nnx.LayerNorm(num_features=msa_channels, rngs=rngs)\n",
    "        self.linear1 = nnx.Linear(msa_channels, n*msa_channels, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(n*msa_channels, msa_channels, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, msa: jax.Array) -> jax.Array:\n",
    "        msa_norm = self.layer_norm(msa)\n",
    "        output = self.linear1(msa_norm)\n",
    "        output = self.linear2(nnx.relu(output))\n",
    "        return output\n",
    "\n",
    "class OuterProductMean(nnx.Module):\n",
    "    def __init__(self, msa_channels, *, rngs: nnx.Rngs, pair_channels, c=32):\n",
    "        self.layer_norm = nnx.LayerNorm(num_features=msa_channels, rngs=rngs)\n",
    "        self.linear_a = nnx.Linear(msa_channels, c, rngs=rngs)\n",
    "        self.linear_b = nnx.Linear(msa_channels, c, rngs=rngs)\n",
    "        self.linear_out = nnx.Linear(c ** 2, pair_channels, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, msa: jax.Array) -> jax.Array:\n",
    "        msa_norm = self.layer_norm(msa)\n",
    "        a,b = self.linear_a(msa_norm), self.linear_b(msa_norm)\n",
    "        outer_product = jnp.einsum('sic,sjd->sijcd', a, b)\n",
    "        mean_outer_product = jnp.mean(outer_product, axis=0)\n",
    "        o = mean_outer_product.reshape(*mean_outer_product.shape[:-2], -1)\n",
    "        z = self.linear_out(o)\n",
    "        return z\n",
    "\n",
    "class TriangleUpdateOutgoing(nnx.Module):\n",
    "    def __init__(self, pair_channels: int, *, rngs: nnx.Rngs, c: int = 128):\n",
    "        self.layer_norm_in = nnx.LayerNorm(num_features=pair_channels, rngs=rngs)\n",
    "        self.linear_a_gate = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_a_val = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_b_gate = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_b_val = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_g = nnx.Linear(pair_channels, pair_channels, rngs=rngs)\n",
    "        self.layer_norm_out = nnx.LayerNorm(num_features=c, rngs=rngs)\n",
    "        self.linear_out = nnx.Linear(c, pair_channels, rngs=rngs)\n",
    "\n",
    "    def __call__(self, z: jax.Array) -> jax.Array:\n",
    "        normed_pairs = self.layer_norm_in(z)\n",
    "        gate_a = nnx.sigmoid(self.linear_a_gate(normed_pairs))\n",
    "        a = gate_a * self.linear_a_val(normed_pairs)\n",
    "        gate_b = nnx.sigmoid(self.linear_b_gate(normed_pairs))\n",
    "        b = gate_b * self.linear_b_val(normed_pairs)\n",
    "        g = nnx.sigmoid(self.linear_g(normed_pairs))\n",
    "        o = jnp.einsum('...ikc,...jkc->...ijc', a, b)\n",
    "        o_norm = self.layer_norm_out(o)\n",
    "        z_delta = g * self.linear_out(o_norm)\n",
    "        return z_delta\n",
    "\n",
    "class TriangleUpdateIncoming(nnx.Module):\n",
    "    def __init__(self, pair_channels: int, *, rngs: nnx.Rngs, c: int = 128):\n",
    "        self.layer_norm_in = nnx.LayerNorm(num_features=pair_channels, rngs=rngs)\n",
    "        self.linear_a_gate = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_a_val = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_b_gate = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_b_val = nnx.Linear(pair_channels, c, rngs=rngs)\n",
    "        self.linear_g = nnx.Linear(pair_channels, pair_channels, rngs=rngs)\n",
    "        self.layer_norm_out = nnx.LayerNorm(num_features=c, rngs=rngs)\n",
    "        self.linear_out = nnx.Linear(c, pair_channels, rngs=rngs)\n",
    "\n",
    "    def __call__(self, z: jax.Array) -> jax.Array:\n",
    "        normed_pairs = self.layer_norm_in(z)\n",
    "        gate_a = nnx.sigmoid(self.linear_a_gate(normed_pairs))\n",
    "        a = gate_a * self.linear_a_val(normed_pairs)\n",
    "        gate_b = nnx.sigmoid(self.linear_b_gate(normed_pairs))\n",
    "        b = gate_b * self.linear_b_val(normed_pairs)\n",
    "        g = nnx.sigmoid(self.linear_g(normed_pairs))\n",
    "        o = jnp.einsum('...kic,...kjc->...ijc', a, b)\n",
    "        o_norm = self.layer_norm_out(o)\n",
    "        z_delta = g * self.linear_out(o_norm)\n",
    "        return z_delta\n",
    "    \n",
    "\n",
    "class TriangleAttentionStart(nnx.Module):\n",
    "    def __init__(self, pair_channels, *, num_heads=8, head_dim=32,rngs: nnx.Rngs):\n",
    "        self.gated_mha = GatedMHA(\n",
    "            embed_dim=pair_channels, \n",
    "            num_heads=num_heads, \n",
    "            head_dim=head_dim,\n",
    "            bias_channels=pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.vmapped_mha = nnx.vmap(\n",
    "            self.gated_mha,\n",
    "            in_axes=(0,None),\n",
    "            out_axes=0,\n",
    "        )\n",
    "    \n",
    "    def __call__(self, z: jax.Array) -> jax.Array:\n",
    "        return self.vmapped_mha(z,z)\n",
    "\n",
    "class TriangleAttentionEnd(nnx.Module):\n",
    "    def __init__(self, pair_channels, num_heads=8, head_dim=32, *, rngs: nnx.Rngs):\n",
    "        self.gated_mha = GatedMHA(\n",
    "            embed_dim=pair_channels, \n",
    "            num_heads=num_heads, \n",
    "            head_dim=head_dim,\n",
    "            bias_channels=pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.vmapped_mha = nnx.vmap(\n",
    "            self.gated_mha,\n",
    "            in_axes=(1,None),\n",
    "            out_axes=1,\n",
    "        )\n",
    "    \n",
    "    def __call__(self, z: jax.Array) -> jax.Array:\n",
    "        return self.vmapped_mha(z,z)\n",
    "\n",
    "class PairsTransition(nnx.Module):\n",
    "    def __init__(self, pair_channels: int, *, rngs: nnx.Rngs, n: int = 4):\n",
    "        self.layer_norm = nnx.LayerNorm(num_features=pair_channels, rngs=rngs)\n",
    "        self.linear1 = nnx.Linear(pair_channels, n*pair_channels, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(n*pair_channels, pair_channels, rngs=rngs)\n",
    "\n",
    "    def __call__(self, z: jax.Array) -> jax.Array:\n",
    "        normed_z = self.layer_norm(z)\n",
    "        a = self.linear1(normed_z)\n",
    "        output = self.linear2(nnx.relu(a))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d912bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoformerBlock(nnx.Module):\n",
    "    def __init__(self, msa_channels: int, pair_channels: int, *, rngs: nnx.Rngs):\n",
    "        self.msa_channels = msa_channels\n",
    "        self.pair_channels = pair_channels\n",
    "        self.row_wise_gated_self_attention_with_pair_bias = MSARowAttentionWithPairBias(\n",
    "            msa_channels=self.msa_channels,\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.column_wise_gated_self_attention = MSAColumnAttention(\n",
    "            msa_channels=self.msa_channels,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.msa_transition = MSATransition(\n",
    "            msa_channels=self.msa_channels,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.outer_product_mean = OuterProductMean(\n",
    "            msa_channels=self.msa_channels,\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.triangular_update_outgoing_edges = TriangleUpdateOutgoing(\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.triangular_update_incoming_edges = TriangleUpdateIncoming(\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.triangle_self_attention_starting_node = TriangleAttentionStart(\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.triangle_self_attention_ending_node = TriangleAttentionEnd(\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.pairs_transition = PairsTransition(\n",
    "            pair_channels=self.pair_channels,\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, msa: jax.Array, pairs: jax.Array) -> jax.Array:\n",
    "        m_residue = self.row_wise_gated_self_attention_with_pair_bias(msa, pairs)\n",
    "        m = msa + m_residue\n",
    "        m += self.column_wise_gated_self_attention(m)\n",
    "        new_msa = m + self.msa_transition(m)\n",
    "        \n",
    "        msa_updates = self.outer_product_mean(msa)\n",
    "        p = pairs + msa_updates\n",
    "        p += self.triangular_update_outgoing_edges(p)\n",
    "        p += self.triangular_update_incoming_edges(p)\n",
    "        p += self.triangle_self_attention_starting_node(p)\n",
    "        p += self.triangle_self_attention_ending_node(p)\n",
    "        new_pairs = p + self.pairs_transition(p)\n",
    "        \n",
    "        return new_msa, new_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single forward pass\n",
    "\n",
    "num_res = 100\n",
    "num_sequences = 50\n",
    "c_msa = 32\n",
    "c_z = 30\n",
    "\n",
    "rngs = nnx.Rngs(params=0, dropout=random.key(1))\n",
    "\n",
    "single_msa = jnp.ones((num_sequences, num_res, c_msa))\n",
    "single_pairs = jnp.ones((num_res, num_res, c_z))\n",
    "\n",
    "evoformer = EvoformerBlock(\n",
    "    msa_channels=c_msa,\n",
    "    pair_channels=c_z,\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "msa, pairs = evoformer(single_msa,single_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dcb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.0006\n",
      "Epoch 1, Loss: 2.9194\n",
      "Epoch 2, Loss: 2.8268\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# a few training loops\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def compute_loss(model: EvoformerBlock, msa: jax.Array, pairs: jax.Array) -> jax.Array:\n",
    "    # dummy targets\n",
    "    target_msa = jnp.zeros_like(msa)\n",
    "    target_pairs = jnp.zeros_like(pairs)\n",
    "    pred_msa, pred_pairs = model(msa, pairs)\n",
    "    msa_loss = jnp.mean((pred_msa - target_msa) ** 2)\n",
    "    pairs_loss = jnp.mean((pred_pairs - target_pairs) ** 2)\n",
    "    return msa_loss + pairs_loss\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, msa_batch, pairs_batch):\n",
    "    def batch_loss_fn(model, msa, pairs):\n",
    "        losses = jax.vmap(compute_loss, in_axes=(None, 0, 0))(model,msa,pairs)\n",
    "        return jnp.mean(losses)\n",
    "    grad_fn = nnx.value_and_grad(batch_loss_fn, wrt=nnx.Param)\n",
    "    loss , grads = grad_fn(model,msa_batch, pairs_batch)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "optimizer = nnx.Optimizer(evoformer, optax.adam(1e-3), wrt=nnx.Param)\n",
    "\n",
    "for epoch in range(3):\n",
    "    key, msa_key, pairs_key = jax.random.split(key, 3)\n",
    "    dummy_msa_batch = jax.random.normal(msa_key, (batch_size, num_sequences, num_res, c_msa))\n",
    "    dummy_pairs_batch = jax.random.normal(pairs_key, (batch_size, num_res, num_res, c_z))\n",
    "    loss = train_step(\n",
    "        evoformer,\n",
    "        optimizer,\n",
    "        dummy_msa_batch,\n",
    "        dummy_pairs_batch,\n",
    "    )\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdef41",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "So first, the only thing I really cared about here is the architecture. I made the forward pass and was able to run the train step for a 1 evoformer block, a batch size of 2, and 3 epochs. It ran faster than I expected (only taking a few seconds with my cpu). Enough so that even if I did scale up to hundreds of residues, 48 evoformer blocks (with recycling), larger batches, more sequences, etc I am a bit suprised that it would cost millions of dollars just for training. That would be an interesting cost analysis to do. Regardless, I got what I needed from alphafold; I don't plan on importing msa representations, building the output pipeline, and draining all of Yale's HPC resouces just to check the completeness of my understanding.\n",
    "\n",
    "I really enjoyed three things about this projects:\n",
    "1. This was my goal from the start. The evoformer was my naive \"shoot for the moon\" project to prove to myself that I understand neural network architectures. And finishing this is extremely fulfilling.\n",
    "2. Thinking about the motivations and meanings of these mathematical operations has become a fun side hobby. For instance, the stated motivation for axial attention is to reduce the context window space, but when you go past that understanding, you also find Google Deepmind modelled the direction of their attention mechanisms with some real meaning. The row wise attention is used to analyze individual sequences or the outgoing edges of a triangle. The column wise attention analyzes the same residue target across sequences or the incoming edges of a triangle. It's beautifully cohesive\n",
    "3. After sifting through jax, keras, and pytorch, I think flax/jax has the most beautiful abstractions. I hated the pytorch implementations of always being careful about batch size dimensions or hardware, and in jax, those things just work. With a few restrictions on how I write my functions (that pretty much all go away when you use nnx), vmap being able to indendently scale across a new dimension, letting me forget about batch size until the last moment. This was also really helpful when I was trying to apply attention row and column wise. vmap made it super easy to implement once I actually understood the system. Oh, and of course jit. I absolutely despise pytorch's to(device) abstraction. I'm not an expert by any means but I feel like it oversimplifies hardware while making code more complex. Wheras @jit just works. I heard pytorch is going towards a more compiler centric route, but until that happens and the research community makes mathematical expressions more idiomatic, I want to stick with jax.\n",
    "\n",
    "As far as anything I want to improve or learn from still, there is one thing I found suprising about the triangular attention: it feels rudimentary. I thought there would be some crazy masking kungfu like we did with the causal masks in the transformer to enforce physical or logical rules. Like there would be a clever trick to mathematically enforce the triangle inequality (I heard alot about that particularly). However, this doesn't happen. The model is supposed to learn those rules for itself. Furthermore, the 'triangle' edges in the attention layer doesn't really seem to be stiched together in a clever way. One of the edges is simply used for the query, one is used for the key, and the last one is just thrown in there as a bias. I recognize that thinking semantically about architecture can be a trap, but I can't help but wonder why the key derived from the edge jk would do a good job attending to an query derived from ij, and how simply addking ki would close that gap. Would the neural network would have learned better connections with the triangular updates if the query was made from the edge ij, and the key was made from combining the edges jk and ki? I highly doubt that Google Deepmind missed this, and the latter approach is likely worse. But if I found out the 'why' of their architecture when they were drafting it, I feel like that would push me towards the next level of AI understanding.\n",
    "\n",
    "With all that said, I finally achieved the goal I set. I feel confident that I can implement any new architecture or at least know where to look to figure out how to build it. In the future, I may come back to explore some other neural networks (diffusion model, adversarial networks, etc), but for now, this is the end of my bootcamp. Thank you for following me in this journey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
