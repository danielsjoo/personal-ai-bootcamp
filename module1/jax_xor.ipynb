{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f5858f",
   "metadata": {},
   "source": [
    "# JAX implementation of XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, value_and_grad, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934bc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [2,2,1] #2 nodes in input, 2 nodes in hidden, 1 in output\n",
    "\n",
    "def init_mlp_params(layer_sizes, key):\n",
    "    params=[]\n",
    "    keys = random.split(key, len(layer_sizes) - 1) # 1 key for each layer\n",
    "    for nin, nout, layer_key in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
    "        w_key, b_key = random.split(layer_key)\n",
    "        layer_params = {\n",
    "            'w': random.normal(w_key, (nout, nin)),\n",
    "            'b': jnp.zeros((nout,))\n",
    "        }\n",
    "        params.append(layer_params)\n",
    "    return params\n",
    "\n",
    "def mlp_apply(params, inputs): #forward pass\n",
    "    x = inputs\n",
    "    for layer_params in params[:-1]:\n",
    "        z = layer_params['w'] @ x + layer_params['b']\n",
    "        x = jax.nn.tanh(z)\n",
    "    final_layer_params = params[-1]\n",
    "    output = final_layer_params['w'] @ x + final_layer_params['b']\n",
    "    return output\n",
    "\n",
    "def batched_loss_fn(params, inputs, targets):\n",
    "    predictions = vmap(mlp_apply, in_axes=(None,0))(params, inputs)\n",
    "    return jnp.mean((jnp.squeeze(predictions)-targets) ** 2)\n",
    "\n",
    "@jit  \n",
    "def batched_train_step(params, inputs, targets, learning_rate):\n",
    "    loss, gradients = value_and_grad(batched_loss_fn)(params, inputs, targets)\n",
    "    # this is a naive way to update, starting next time we will use an optimizer\n",
    "    new_params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, gradients\n",
    "    )\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb8138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_loss_fn(params, inputs, target):\n",
    "    prediction = mlp_apply(params, inputs)\n",
    "    return (jnp.squeeze(prediction) - target) ** 2\n",
    "\n",
    "@jit\n",
    "def sgd_train_step(params, inputs, target, learning_rate):\n",
    "    loss, gradient = value_and_grad(sgd_loss_fn)(params, inputs, target)\n",
    "    new_params = jax.tree_util.tree_map(\n",
    "        lambda p,g: p - learning_rate * g, params, gradient\n",
    "    )\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fa6615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Batched Gradient Descent____\n",
      "Epoch 0, Loss: 2.8005\n",
      "Epoch 100, Loss: 0.2331\n",
      "Epoch 200, Loss: 0.1599\n",
      "Epoch 300, Loss: 0.0455\n",
      "Epoch 400, Loss: 0.0029\n",
      "Epoch 500, Loss: 0.0001\n",
      "Epoch 600, Loss: 0.0000\n",
      "Epoch 700, Loss: 0.0000\n",
      "Epoch 800, Loss: 0.0000\n",
      "Epoch 900, Loss: 0.0000\n",
      "\n",
      "____SGD Gradient Descent____\n",
      "Epoch 0, Average Loss: 1.2757\n",
      "Epoch 100, Average Loss: 0.2412\n",
      "Epoch 200, Average Loss: 0.2096\n",
      "Epoch 300, Average Loss: 0.1463\n",
      "Epoch 400, Average Loss: 0.0538\n",
      "Epoch 500, Average Loss: 0.0048\n",
      "Epoch 600, Average Loss: 0.0002\n",
      "Epoch 700, Average Loss: 0.0000\n",
      "Epoch 800, Average Loss: 0.0000\n",
      "Epoch 900, Average Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(42)\n",
    "batched_params = init_mlp_params(layer_sizes, key)\n",
    "sgd_params = init_mlp_params(layer_sizes, key)\n",
    "batch_learning_rate = 0.1\n",
    "sgd_learning_rate = 0.02\n",
    "epochs = 1000\n",
    "X_train = jnp.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "Y_train = jnp.array([0,1,1,0])\n",
    "\n",
    "print(\"____Batched Gradient Descent____\")\n",
    "for epoch in range(epochs):\n",
    "    batched_params, loss = batched_train_step(batched_params, X_train, Y_train, batch_learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "print(\"\\n____SGD Gradient Descent____\")\n",
    "for epoch in range(epochs):\n",
    "    key, subkey = random.split(key)\n",
    "    shuffled_indices = random.permutation(subkey, 4)\n",
    "    X_shuffled = X_train[shuffled_indices]\n",
    "    Y_shuffled = Y_train[shuffled_indices]\n",
    "    for x_input, y_target in zip(X_shuffled, Y_shuffled):\n",
    "        sgd_params, loss = sgd_train_step(sgd_params, x_input, y_target, sgd_learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "            # Use vmap to efficiently apply the model to the entire batch of inputs\n",
    "            predictions = vmap(mlp_apply, in_axes=(None, 0))(sgd_params, X_train)\n",
    "            # Calculate the mean squared error over the whole dataset\n",
    "            epoch_loss = jnp.mean((jnp.squeeze(predictions) - Y_train) ** 2)\n",
    "            print(f\"Epoch {epoch}, Average Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f423e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0. 0.], Prediction: 0.0, Actual: 0\n",
      "Input: [0. 1.], Prediction: 1.0, Actual: 1\n",
      "Input: [1. 0.], Prediction: 1.0, Actual: 1\n",
      "Input: [1. 1.], Prediction: 0.0, Actual: 0\n",
      "\n",
      "Batched raw model outputs (logits):\n",
      "[8.3446503e-07 9.9999881e-01 9.9999851e-01 5.9604645e-07] \n",
      "\n",
      "Input: [0. 0.], Prediction: 0.0, Actual: 0\n",
      "Input: [0. 1.], Prediction: 1.0, Actual: 1\n",
      "Input: [1. 0.], Prediction: 1.0, Actual: 1\n",
      "Input: [1. 1.], Prediction: 0.0, Actual: 0\n",
      "\n",
      "SGD raw model outputs (logits):\n",
      "[7.3909760e-06 9.9999356e-01 9.9999011e-01 6.0796738e-06]\n"
     ]
    }
   ],
   "source": [
    "final_predictions = jnp.squeeze(vmap(mlp_apply, in_axes=(None,0))(batched_params, X_train))\n",
    "binary_predictions = jnp.round(final_predictions)\n",
    "for i, (inputs, pred, actual) in enumerate(zip(X_train, binary_predictions, Y_train)):\n",
    "    print(f\"Input: {inputs}, Prediction: {pred}, Actual: {actual}\")\n",
    "\n",
    "# You can also see the raw output values\n",
    "print(\"\\nBatched raw model outputs (logits):\")\n",
    "print(f\"{final_predictions} \\n\")\n",
    "\n",
    "final_predictions = jnp.squeeze(vmap(mlp_apply, in_axes=(None,0))(sgd_params, X_train))\n",
    "binary_predictions = jnp.round(final_predictions)\n",
    "for i, (inputs, pred, actual) in enumerate(zip(X_train, binary_predictions, Y_train)):\n",
    "    print(f\"Input: {inputs}, Prediction: {pred}, Actual: {actual}\")\n",
    "\n",
    "# You can also see the raw output values\n",
    "print(\"\\nSGD raw model outputs (logits):\")\n",
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
