{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e3e356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/danieljoo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from gpt.decoder import DecoderOnlyTransformer\n",
    "from gpt.position_encoder import PositionalEncoding\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "embed_dim = 150\n",
    "max_len = 75\n",
    "num_transformers = 6\n",
    "num_heads = 5\n",
    "dense_dim = 256\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim = embed_dim,\n",
    "        num_transformers = num_transformers,\n",
    "        num_heads = num_heads,\n",
    "        dense_dim = dense_dim,\n",
    "        pad_token_id = PAD_TOKEN_ID\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = embed_dim,\n",
    "            padding_idx = pad_token_id,\n",
    "        )\n",
    "        self.position_encoding = PositionalEncoding(\n",
    "            embed_dim = embed_dim,\n",
    "            max_len = max_len,\n",
    "        )\n",
    "        self.transformer_stack = nn.ModuleList([\n",
    "            DecoderOnlyTransformer(\n",
    "                embed_dim = embed_dim,\n",
    "                num_heads = num_heads,\n",
    "                dense_dim = dense_dim,\n",
    "        ) for _ in range(num_transformers)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features = embed_dim,\n",
    "            out_features = vocab_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        key_padding_mask = (x == PAD_TOKEN_ID)\n",
    "        x = self.token_embed(x)\n",
    "        x = self.position_encoding(x)\n",
    "        for transformer in self.transformer_stack:\n",
    "            x = transformer(x, key_padding_mask = key_padding_mask)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x #loss will be computed from logits for stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d180e3",
   "metadata": {},
   "source": [
    "# Inference phase of gpt trained on cluster\n",
    "The meat of this project like the architecture is in the other notebook. This is solely the inference I get from scaling the model, which in this case means simply running the training on a hpc compute node (that has a relatively high end gpu) as opposed to my mac. The exact hyperparameters were not fine tuned and just guestimated to make the most the compute node's vram and make training last like an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2d7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (token_embed): Embedding(30522, 150, padding_idx=0)\n",
       "  (position_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_stack): ModuleList(\n",
       "    (0-5): 6 x DecoderOnlyTransformer(\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=150, out_features=150, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=150, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=150, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=150, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length, temperature, pad_token_id):\n",
    "    model.eval()\n",
    "    input_ids_list = tokenizer.encode(prompt, truncation=True, max_length=max_length - 1)\n",
    "    \n",
    "    # Use the pad_token_id from the model's config\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    generated_ids = torch.tensor([input_ids_list], device=device, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(input_ids_list)):\n",
    "            current_len = generated_ids.size(1)\n",
    "            # The model was trained with fixed-size input (max_len), so we must pad the input\n",
    "            padded_input = torch.full((1, max_len), pad_token_id, device=device, dtype=torch.long)\n",
    "            padded_input[:, :current_len] = generated_ids\n",
    "            \n",
    "            logits = model(padded_input)\n",
    "\n",
    "            # Get the logits for the last token in the sequence\n",
    "            next_token_logits = logits[:, current_len - 1, :]\n",
    "            \n",
    "            scaled_logits = next_token_logits / temperature\n",
    "            \n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "            if eos_token_id and next_token_id.item() == eos_token_id:\n",
    "                break\n",
    "    generated_text = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    vocab_size=30522,\n",
    "    embed_dim=150,\n",
    "    num_heads=5,\n",
    "    num_transformers=6,\n",
    "    dense_dim=256,\n",
    "    pad_token_id=0\n",
    ")\n",
    "model.load_state_dict(torch.load(\"my_trained_gpt_model/model_state_dict.pth\", map_location=torch.device('cpu')))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d70843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Generation ---\n",
      "Prompt: 'In 2001,'\n",
      "Generated text: 'in 2001, summoned monttry - but condemned ganga throughout the show.'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In 2001,\"\n",
    "print(\"\\n--- Starting Generation ---\")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "\n",
    "generated_text = generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length=50,\n",
    "    temperature=1,\n",
    "    pad_token_id=0\n",
    ")\n",
    "\n",
    "print(f\"Generated text: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cdca4",
   "metadata": {},
   "source": [
    "## Thoughts/Guesses on scaling\n",
    "\n",
    "If I were to make a hypothesis about how to adjust hyperparameters and use more resources, I would have to increase and manage:\n",
    "1. Dataset size. I restricted my train data to 50,000 articles which leads to around a million sentences. But that's still only a small fraction of the complete size of the dataset. But of course, iterating until I hit the cap probably would have increased my dataset by 100x but also my training time by 100x, which is impossible without paralleliation/reducing epochs since the max job length is 2 days. It's not related to scaling, but I might also try a different dataset that isn't as formal. For instance, Wikipedia is pretty much entirely third person, which is not a good representation of the kind of language that people bring to chatbots.\n",
    "2. Parallelized Distributed Training: The compute node comes with up to 4 gpus. That's potentially 4x more data processed. This does require care though because doing something that would be mathematically equivalent to increasing batch size could change how it converges.\n",
    "3. Learning Rate and Epochs: If I implemented the above, I could probably get away with a faster learning rate since a larger batch would lead to a less noisy gradient. That could reduce the epochs (I'm finding my current configuration to plateau around 20/30 epochs in anyways)\n",
    "4. Number of attention heads, transformer layers, dense dimension: I think with more data, we can probably increase them but the scale of that increase is not something I have the experience to estimate\n",
    "\n",
    "So overall, my best guess for how to go about a second attempt would be: parallelize (decrease training time) -> bigger batches (decrease training time) -> faster learning rate (decrease training time) -> fewer epochs (decrease training time) -> more complex model hyperparameters (increase training time) -> more data until I reach the cap (increase training time).\n",
    "\n",
    "Eh, but that's a lesson for next time, though. Right now, I'm satisfied with these mediocre inputs for my pedagogical bootcamp, and I'll save the experimental testing tools for next time when the problem is more well defined and simple (image classification as opposed to word predictions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
