{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4536e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/danieljoo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import re\n",
    "from itertools import islice\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from gpt.decoder import DecoderOnlyTransformer\n",
    "from gpt.position_encoder import PositionalEncoding\n",
    "\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5659c5",
   "metadata": {},
   "source": [
    "# Pytorch Implementation of GPT2\n",
    "\n",
    "Here, I will recreate the decoder only transformer architecture of gpt, and train it on wikipedia.\n",
    "\n",
    "https://docs.pytorch.org/tutorials/beginner/basics/intro.html \\\n",
    "https://www.youtube.com/watch?v=bQ5BoolX9Ag \n",
    "\n",
    "<img src =\"https://miro.medium.com/v2/resize:fit:1400/1*qTjjAvXmrSaRdN1LODLVGA.png\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7ca1d",
   "metadata": {},
   "source": [
    "## Model Declaration\n",
    "\n",
    "I created the decoder and positional encoder in modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48989a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "max_len = 50\n",
    "num_transformers = 6\n",
    "num_heads = 5\n",
    "dense_dim = 256\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim = embed_dim,\n",
    "        num_transformers = num_transformers,\n",
    "        num_heads = num_heads,\n",
    "        dense_dim = dense_dim,\n",
    "        pad_token_id = PAD_TOKEN_ID):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = embed_dim,\n",
    "            padding_idx = pad_token_id,\n",
    "        )\n",
    "        self.position_encoding = PositionalEncoding(\n",
    "            embed_dim = embed_dim,\n",
    "            max_len = max_len,\n",
    "        )\n",
    "        self.transformer_stack = nn.ModuleList([\n",
    "            DecoderOnlyTransformer(\n",
    "                embed_dim = embed_dim,\n",
    "                num_heads = num_heads,\n",
    "                dense_dim = dense_dim,\n",
    "        ) for _ in range(num_transformers)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features = embed_dim,\n",
    "            out_features = vocab_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        key_padding_mask = (x == PAD_TOKEN_ID)\n",
    "        x = self.token_embed(x)\n",
    "        x = self.position_encoding(x)\n",
    "        for transformer in self.transformer_stack:\n",
    "            x = transformer(x, key_padding_mask = key_padding_mask)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x #loss will be computed from logits for stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266e472",
   "metadata": {},
   "source": [
    "## Data Loading and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ae9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train',streaming=True)\n",
    "shuffled_stream = wiki_dataset.shuffle(seed=42, buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96888910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from batch 0:\n",
      "  - Item 0: '() is a German indie-pop band from Berlin that was founded in 2006 under the name Fluchtweg.'\n",
      "  - Item 1: 'In 2021, his second wife accused him of raping her.'\n",
      "  - Item 2: 'See also Roads in Ireland National primary road National secondary road References Regional roads in the Republic of Ireland Roads in County Louth'\n",
      "  - Item 3: 'On February 18, 2016, a detailed agreement was signed with a German partner, which specified the terms of cooperation between ZM Bumar-Łabędy and Rheinmetall.'\n",
      "  - Item 4: 'The modernisation is currently being carried out in cooperation with Rheinmetall and the Polish Armaments Group ().'\n",
      "  - Item 5: 'The book was positively received by African-American and communist media of the time.'\n",
      "  - Item 6: 'She was the only child of the Inland Revenue tax officer Thomas John Tindale and his wife Princess May, née Uttin.'\n",
      "  - Item 7: 'Background Angela Davis is a Marxist feminist author born in Alabama, United States, in 1944.'\n",
      "Sentences from batch 1:\n",
      "  - Item 0: 'The Leopard 2PL is a main battle tank used by the Polish Armed Forces, and is a modernized version of the older Leopard 2A4 tank, phased out by Germany and first acquired by Poland in the 2000s.'\n",
      "  - Item 1: 'Notable people with the surname include: Claudio Bergamin, Chilean/Italian fantasy artist Francisco Bergamín y García (1855–1937), Spanish lawyer, economist and politician José Bergamín (1895–1983), Spanish writer, essayist, poet and playwright Luciano Bergamin (born 1944), Italian clergyman and Roman Catholic Bishop of Nova Iguaçu in Brazil Massimo Bergamin (born 1964), Italian politician See also Bergamini'\n",
      "  - Item 2: 'AEK Women's Water Polo Club is the women's water polo department of the major Greek multi-sport club, AEK Sports Club, based in Athens, Greece.'\n",
      "  - Item 3: 'Driver awareness was improved by installing a KDN-1 Nyks day / night reversing camera also PCO production.'\n",
      "  - Item 4: 'Robert Chrisman of The Black Scholar found it \"a brilliant and comprehensive collection [...] on the revolutionary struggle of black prisoners against the repressive machinery of prisons and the court system\".'\n",
      "  - Item 5: 'The contract was extended until the end of July 2023.'\n",
      "  - Item 6: 'The tank is designed to master and maintain the area, and support fire from deck weapons of mechanized and motorized subunits, in all weather conditions, both during the day and at night.'\n",
      "  - Item 7: 'In 1970, the Soledad Brothers—George Jackson, Fleeta Drumgo, and John Clutchette—were charged with murdering a white prison guard, whose shooting of three black prisoners involved in a fist fight was judged \"justifiable homicide\".'\n",
      "Sentences from batch 2:\n",
      "  - Item 0: 'In August 2018, the vehicle was sent to the Military Armored and Automotive Institute in Sulejówek, where it underwent further national tests in Poland.'\n",
      "  - Item 1: 'References 1971 non-fiction books Books about African-American history Books by Angela Davis Memoirs of imprisonment Works about American prisons'\n",
      "  - Item 2: 'Finally, modernization program was included in the \"Technical Modernization Plan of the Polish Armed Forces for 2013-2022\" () in December 2012.'\n",
      "  - Item 3: 'He belongs to a Bengali Muslim family of Qadis originally from the village of Bhirich Khan, in Louhajang, Munshiganj.'\n",
      "  - Item 4: 'History The women's water polo team of AEK started its efforts from the A2 Greek Women's Water Polo League in the 2019–20 season.'\n",
      "  - Item 5: 'However Jharna has charged him with rape and detention and stated that they are not married.'\n",
      "  - Item 6: 'In 1951 Tindale moved to the ministry's development group to develop prototypes for the new generation of spacious school buildings for quick reconfiguration site wide by sliding partitions.'\n",
      "  - Item 7: 'At the registration of her birth, Tindale's name was not recorded but her naming certificate had her name as Sheila Randall Patricia.'\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 10\n",
    "batch_size = 64\n",
    "\n",
    "def clean_wiki_text(text):\n",
    "    # removes wikipedia headers\n",
    "    text = re.sub(r'={2,}.*?={2,}', '', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "class WikiSentenceDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        super().__init__()\n",
    "        self.hf_dataset = hf_dataset\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for example in self.hf_dataset:\n",
    "            clean_text = clean_wiki_text(example['text'])\n",
    "            if clean_text:\n",
    "                sentences = nltk.sent_tokenize(clean_text)\n",
    "                for sentence in sentences:\n",
    "                    yield sentence\n",
    "                    \n",
    "class ShufflingIterableDataset(IterableDataset):\n",
    "    def __init__(self, source_dataset, buffer_size, seed):\n",
    "        super().__init__()\n",
    "        self.source_dataset = source_dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        self.seed = seed\n",
    "    \n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        source_iterator = iter(self.source_dataset)\n",
    "        shuffle_buffer = list(islice(source_iterator, self.buffer_size))\n",
    "        # first loop replaces given item with source_iterator addition\n",
    "        for item in source_iterator:\n",
    "            idx = rng.randint(0, self.buffer_size-1)\n",
    "            yield shuffle_buffer[idx]\n",
    "            shuffle_buffer[idx] = item\n",
    "        # second loop flushes the current buffer when source_iterator is dry\n",
    "        rng.shuffle(shuffle_buffer)\n",
    "        for item in shuffle_buffer:\n",
    "            yield item\n",
    "            \n",
    "            \n",
    "train_stream = shuffled_stream.take(TRAIN_SIZE)\n",
    "test_stream = shuffled_stream.skip(TRAIN_SIZE).take(TEST_SIZE)\n",
    "\n",
    "train_stream = WikiSentenceDataset(train_stream)\n",
    "test_stream = WikiSentenceDataset(test_stream)\n",
    "\n",
    "train_dataset = ShufflingIterableDataset(train_stream, buffer_size=1000, seed=42)\n",
    "test_dataset = ShufflingIterableDataset(test_stream, buffer_size=1000, seed=42)\n",
    "\n",
    "data_loader_plain = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 4\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(data_loader_plain):\n",
    "    print(f\"Sentences from batch {i}:\")\n",
    "    for j, sentence in enumerate(batch):\n",
    "        print(f\"  - Item {j}: '{sentence}'\")\n",
    "        \n",
    "    if i>1:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "collate_fn = lambda sentences: tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',   \n",
    "        truncation = True,\n",
    "        max_length = max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae841dab",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d656508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4it [00:07,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 1it [00:06,  6.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 10.4174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4it [00:07,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.2598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 1it [00:05,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 10.3034\n",
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = NeuralNetwork(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_loader_size = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        train_loader_size += 1\n",
    "        b = batch['input_ids']\n",
    "        b = b.to(device)\n",
    "        inputs = b[:,:-1]\n",
    "        targets = b[:,1:]\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits.reshape(-1,vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / train_loader_size\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        test_loader_size = 0\n",
    "        for batch in tqdm(test_loader, desc=\"Validation\"):\n",
    "            b = batch['input_ids']\n",
    "            test_loader_size += 1\n",
    "            b = b.to(device)\n",
    "            inputs = b[:,:-1]\n",
    "            targets = b[:,1:]\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / test_loader_size\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404780e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2703ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.1):\n",
    "    model.eval()\n",
    "    input_ids_list = tokenizer.encode(prompt, truncation=True, max_length=max_length - 1)\n",
    "    \n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    generated_ids = torch.tensor([input_ids_list], device=device, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(input_ids_list)):\n",
    "            current_len = generated_ids.size(1)\n",
    "            padded_input = torch.full((1, max_len), pad_token_id, device=device, dtype=torch.long)\n",
    "            padded_input[:, :current_len] = generated_ids\n",
    "            \n",
    "            logits = model(padded_input)\n",
    "\n",
    "            next_token_logits = logits[:, current_len - 1, :]\n",
    "            \n",
    "            scaled_logits = next_token_logits / temperature\n",
    "            \n",
    "            probabilities = F.softmax(scaled_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "            if eos_token_id and next_token_id.item() == eos_token_id:\n",
    "                break\n",
    "    generated_text = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af6b216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation...\n",
      "Prompt: The most prominent figure of the 20th century is\n",
      "\n",
      "Output: the most prominent figure of the 20th century is cost children gleam whereupon dancer tastes postal danielle timeline bankrupt bullock yellowstone jasmine hinduism [unused827] widowvus outragemblingart♦ trembling extensive scubasure illumination [unused719] flavor croatianشady infections nurses sue marin ن kimball assaults charts\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The most prominent figure of the 20th century is\"\n",
    "print(\"Starting generation...\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "generated_paragraph = generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_length = 50,\n",
    "    temperature = .5\n",
    ")\n",
    "\n",
    "print(f\"Output: {generated_paragraph}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fc7d4",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "So it turns out the Decoder architecture is really simple to implement on PyTorch. The framework kinda mixes and matches what I learned in Keras and Jax, choosing to be more abstract or more imperative on some things.\n",
    "\n",
    "But UGH training a large model like this takes a ton of time and learning to do it remotely on hardware with actual gpus was also a pain in the butt. I'm worried that the common theme of actual ML engineering is that model creation and architecture is easy; scaling, however, is hard. The actual takeaways here come from simply trying to run this remotely: \n",
    "\n",
    "- virtual python enviornments\n",
    "- picking nodes and sending jobs to slurm\n",
    "- rerouting IO and making sure to recieve model outputs\n",
    "- mointoring model training progress\n",
    "\n",
    "But it's absolutely worth because on the cluster, I can train this model with 4x the batch size while still have 6x the iterations per second. Doing this on 50x the data, and allowing the training to run for a little over an hour (which is doable because its remote) brings my validation loss from like 7 to 1.5.\n",
    "\n",
    "And that doesn't even get to distributed learning, utilizing more workers on the CPU, machine learning experimentation platforms (like tensorboard), and figuring out how to refactor all this legibly. For the next one, I'll focus on an easier problem like CIFAR 10 and I promise it'll be cleaner and nicer looking.\n",
    "\n",
    "Pytorch and Hugging Face comes with a lot of interesting tools. For instance, I found it odd how you have to be imperative with device usage in pytorch and hugging face provided a lot of tools I needed for the NLP dataset generation.\n",
    "The most complicated part in this notebook was honestly the pytorch dataloader, which seems like a pretty convenient tool that I can just subclass to create an iterable for my training loop. It was a challenge getting it to work with a stream of data rather than just loading it all into ram as I have done before but luckily that should be useful for the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
