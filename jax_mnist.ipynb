{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b720797f",
   "metadata": {},
   "source": [
    "# JAX Implementation of MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb0f48",
   "metadata": {},
   "source": [
    "## Import Modules and Data\n",
    "I will bring in the data with torchvision. For this module, I'll load the data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31746509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import optax\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, value_and_grad, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8a3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(root = './data/mnist', train=True, download=True)\n",
    "test_data = datasets.MNIST(root = './data/mnist', train=False, download=True)\n",
    "train_images_np = training_data.data.numpy()\n",
    "train_labels_np = training_data.targets.numpy()\n",
    "test_images_np = test_data.data.numpy()\n",
    "test_labels_np = test_data.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06314c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_jnp = jnp.array(train_images_np)\n",
    "train_labels_jnp = jnp.array(train_labels_np)\n",
    "\n",
    "test_images_jnp = jnp.array(test_images_np)\n",
    "test_labels_jnp = jnp.array(test_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4225ab7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images_jnp shape: (60000, 28, 28)\n",
      "train_labels_jnp shape: (60000,)\n",
      "test_images_jn shape: (10000, 28, 28)\n",
      "test_labels_jnp shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_images_jnp shape: {train_images_jnp.shape}\")\n",
    "print(f\"train_labels_jnp shape: {train_labels_jnp.shape}\")\n",
    "print(f\"test_images_jn shape: {test_images_jnp.shape}\")\n",
    "print(f\"test_labels_jnp shape: {test_labels_jnp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776fcf6",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "- Normalization: Bring range to [0,1]\n",
    "We will skip standardization, which is more for pre-trained models\n",
    "- Flatten: instead of 28*28 images, we are using (784,) vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800f11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images_jnp shape: (60000, 784)\n",
      "train_labels_jnp shape: (60000,)\n",
      "test_images_jn shape: (10000, 784)\n",
      "test_labels_jnp shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_images_jnp = train_images_jnp.astype(jnp.float32) / 255.0\n",
    "test_images_jnp = test_images_jnp.astype(jnp.float32) / 255.0\n",
    "train_images_jnp = train_images_jnp.reshape(train_images_jnp.shape[0], 784)\n",
    "test_images_jnp = test_images_jnp.reshape(test_images_jnp.shape[0], 784)\n",
    "\n",
    "print(f\"train_images_jnp shape: {train_images_jnp.shape}\")\n",
    "print(f\"train_labels_jnp shape: {train_labels_jnp.shape}\")\n",
    "print(f\"test_images_jn shape: {test_images_jnp.shape}\")\n",
    "print(f\"test_labels_jnp shape: {test_labels_jnp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eb878",
   "metadata": {},
   "source": [
    "## Create Forward pass and Loss in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7d6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_params(layer_sizes, key):\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_sizes) - 1)\n",
    "    for nin, nout, layer_key in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
    "        w_key, b_key = random.split(layer_key)\n",
    "        layer_params = {\n",
    "            'w': random.normal(w_key, (nout, nin)),\n",
    "            'b': jnp.zeros((nout,))\n",
    "        }\n",
    "        params.append(layer_params)\n",
    "    return params\n",
    "\n",
    "#inputs shape is (784,)\n",
    "def mlp_apply(params, inputs):\n",
    "    x = inputs\n",
    "    for layer_params in params[:-1]:\n",
    "        z = layer_params['w'] @ x + layer_params['b']\n",
    "        x = jax.nn.relu(z)\n",
    "    final_layer_params = params[-1]\n",
    "    output = final_layer_params['w'] @ x + final_layer_params['b']\n",
    "    return output\n",
    "\n",
    "#inputs shape is (128, 784)\n",
    "def batched_loss_fn(params, inputs, targets):\n",
    "    predictions = vmap(mlp_apply, in_axes=(None,0))(params, inputs)\n",
    "    return jnp.mean(optax.losses.softmax_cross_entropy_with_integer_labels(predictions, labels=targets))\n",
    "\n",
    "weight_decay = 1e-4  # A good starting point\n",
    "solver = optax.adamw(learning_rate=0.001, weight_decay=weight_decay)\n",
    "@jit \n",
    "def batched_train_step(params, inputs, targets, opt_state):\n",
    "    loss, gradients = value_and_grad(batched_loss_fn)(params, inputs, targets)\n",
    "    updates, opt_state = solver.update(gradients, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b39cc",
   "metadata": {},
   "source": [
    "## Initalize Model and test the forward pass and batched loss\n",
    "\n",
    "- The single prediction should look like 10 random numbers\n",
    "- The Cross Entropy Loss function should hover around -ln(1/10) for each number, around 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b58f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single prediction (one hot encoded vector): \n",
      "[  391.79626   1618.063      696.4814     367.7426    2863.2422\n",
      "  2379.1455   -1363.0208    -735.86993     74.458496   840.37585 ]\n",
      "\n",
      "cross entropy loss of numbers above for batch of 128: \n",
      "1868.5386962890625\n"
     ]
    }
   ],
   "source": [
    "# Chatgpt told me to use these layer sizes, optimizer, and learning rate. Since this is not a hyperparameter tuning exercise, I will just use them\n",
    "layer_sizes = [784, 512, 256, 10]\n",
    "key = random.PRNGKey(42)\n",
    "params = init_mlp_params(layer_sizes, key)\n",
    "opt_state = solver.init(params)\n",
    "\n",
    "#single pass\n",
    "single_prediction = mlp_apply(params, train_images_jnp[0])\n",
    "print(f'single prediction (one hot encoded vector): \\n{single_prediction}')\n",
    "\n",
    "#batched loss\n",
    "batch_size = 128\n",
    "single_batched_loss = batched_loss_fn(params, train_images_jnp[0:batch_size], train_labels_jnp[0:batch_size])\n",
    "print(f'\\ncross entropy loss of numbers above for batch of {batch_size}: \\n{single_batched_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addb449",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c7f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training: 0.062300000339746475\n",
      "Accuracy after epoch 0: 0.8841000199317932\n",
      "Accuracy after epoch 1: 0.9138000011444092\n",
      "Accuracy after epoch 2: 0.9204999804496765\n",
      "Accuracy after epoch 3: 0.9311000108718872\n",
      "Accuracy after epoch 4: 0.9369000196456909\n",
      "Accuracy after epoch 5: 0.9373000264167786\n",
      "Accuracy after epoch 6: 0.9424999952316284\n",
      "Accuracy after epoch 7: 0.9426000118255615\n",
      "Accuracy after epoch 8: 0.945900022983551\n",
      "Accuracy after epoch 9: 0.9402999877929688\n",
      "Accuracy after epoch 10: 0.9459999799728394\n",
      "Accuracy after epoch 11: 0.9473999738693237\n",
      "Accuracy after epoch 12: 0.9531999826431274\n",
      "Accuracy after epoch 13: 0.9539999961853027\n",
      "Accuracy after epoch 14: 0.9534000158309937\n",
      "Accuracy after epoch 15: 0.9545000195503235\n",
      "Accuracy after epoch 16: 0.9556999802589417\n",
      "Accuracy after epoch 17: 0.9591000080108643\n",
      "Accuracy after epoch 18: 0.957099974155426\n",
      "Accuracy after epoch 19: 0.9567000269889832\n",
      "Accuracy after epoch 20: 0.9589999914169312\n",
      "Accuracy after epoch 21: 0.9587000012397766\n",
      "Accuracy after epoch 22: 0.9569000005722046\n",
      "Accuracy after epoch 23: 0.9587000012397766\n",
      "Accuracy after epoch 24: 0.9602000117301941\n",
      "Accuracy after epoch 25: 0.9613000154495239\n",
      "Accuracy after epoch 26: 0.9606000185012817\n",
      "Accuracy after epoch 27: 0.9610999822616577\n",
      "Accuracy after epoch 28: 0.9624000191688538\n",
      "Accuracy after epoch 29: 0.960099995136261\n"
     ]
    }
   ],
   "source": [
    "#redeclaring for idempotency\n",
    "layer_sizes = [784, 512, 256, 10]\n",
    "key = random.PRNGKey(42)\n",
    "params = init_mlp_params(layer_sizes, key)\n",
    "opt_state = solver.init(params)\n",
    "\n",
    "epochs = 30\n",
    "shuffling_key = random.PRNGKey(42)\n",
    "n_train_samples = train_images_jnp.shape[0]\n",
    "\n",
    "def test_model(params, test_data, test_labels):\n",
    "    predictions = vmap(mlp_apply, in_axes=(None,0))(params, test_data)\n",
    "    predicted_labels = jnp.argmax(predictions, axis=1)\n",
    "    correct_count = jnp.sum(predicted_labels == test_labels)\n",
    "    return correct_count / test_labels.shape[0]\n",
    "\n",
    "accuracy = test_model(params, test_images_jnp, test_labels_jnp)\n",
    "print(f'Accuracy before training: {accuracy}')\n",
    "for epoch in range(epochs):\n",
    "    shuffling_key, subkey = random.split(shuffling_key)\n",
    "    permuted_indices = random.permutation(subkey, n_train_samples)\n",
    "    for i in range(0, n_train_samples, batch_size):\n",
    "        batch_indices = batch_indices = permuted_indices[i : i + batch_size]\n",
    "        params, opt_state, loss = batched_train_step(params, train_images_jnp[batch_indices], train_labels_jnp[batch_indices], opt_state)\n",
    "    accuracy = test_model(params, test_images_jnp, test_labels_jnp)\n",
    "    print(f'Accuracy after epoch {epoch}: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f04d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy: 0.960099995136261\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(params, test_images_jnp, test_labels_jnp)\n",
    "print(f'final accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4fa11",
   "metadata": {},
   "source": [
    "## Some Reflections\n",
    "\n",
    "So where can I improve...\n",
    "\n",
    "1) Data loading: loading all the data into memory will crash for bigger datasets. I can fix this with a data loader, which is an abstraction I will start using in the next module.\n",
    "2) From now on, I can encapsulate params and opt_state into one TrainState named tuple. This will ease complexity as more training things come into play like batch normalization statistics\n",
    "\n",
    "Review:\n",
    "\n",
    "1) Numpy operations. That sum(==) statement I had to copy from chatgpt, and it should be idiomatically engrained\n",
    "2) jnp axes. I'm not exactly sure what vmap(in_axes(None, 0)) or axis=1 really means.\n",
    "3) Keys. Understand exactly what splitting keys, does because now it's just a magical random number generator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
