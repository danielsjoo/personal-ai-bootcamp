{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af4aa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d1bcf",
   "metadata": {},
   "source": [
    "# English to Spanish Translator\n",
    "\n",
    "Building on the previous encoder model that predicted imdb sentiment, I will build an encoder and decoder for the full transformer and apply it to the english/spanish dataset. Hopefully I will have a good enough grasp after this to detour to CNNs and then move up to Axial Trasformers.\n",
    "\n",
    "This time I am just following [this tutorial](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/) by studying the code, transcribing the architecture onto diagrams and notes, and doing my best to recreate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084e20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng_extracted\" / \"spa-eng\"/\"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c817328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n",
      "shape of inputs[\"encoder_inputs\"]: (64, 20)\n",
      "shape of inputs[\"decoder_inputs\"]: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "with open(text_file) as file:\n",
    "    text_pairs = []\n",
    "    for line in file:\n",
    "        eng, spa = line.split(\"\\t\")\n",
    "        spa = \"[start]\" + spa + \" [end]\"\n",
    "        text_pairs.append((eng,spa))\n",
    "        \n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples : ]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "VOCAB_SIZE = 15000\n",
    "SEQUENCE_LEN = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf_strings.lower(input_string)\n",
    "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode= 'int',\n",
    "    output_sequence_length= SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode= 'int',\n",
    "    output_sequence_length= SEQUENCE_LEN + 1,\n",
    "    standardize = custom_standardization\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)\n",
    "\n",
    "def format_dataset(eng,spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": spa[:,:-1]\n",
    "        },\n",
    "        spa[:,1:],\n",
    "    )\n",
    "    \n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.cache().shuffle(2024).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'shape of inputs[\"encoder_inputs\"]: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'shape of inputs[\"decoder_inputs\"]: {inputs[\"decoder_inputs\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_len, embed_dim, vocab_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_len = sequence_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenEmbedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.positionEmbedding = layers.Embedding(sequence_len, embed_dim)\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return ops.not_equal(inputs, 0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        token_embedding = self.tokenEmbedding(inputs)\n",
    "        positions = ops.arange(0,self.sequence_len,1)\n",
    "        position_embedding = self.positionEmbedding(positions)\n",
    "        return (token_embedding + position_embedding)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"sequence_len\": self.sequence_len,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = embed_dim // num_heads\n",
    "        self.mha_layer = layers.MultiHeadAttention(\n",
    "            key_dim = self.key_dim,\n",
    "            num_heads = self.num_heads\n",
    "        )\n",
    "        self.normalization_layer = layers.LayerNormalization()\n",
    "        self.normalization_layer2 = layers.LayerNormalization()\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(self.dense_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        \n",
    "    def call(self,inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = ops.cast(mask[:,None,:], dtype='int32')\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        mha_output = self.mha_layer(\n",
    "            query = inputs,\n",
    "            key = inputs,\n",
    "            value = inputs,\n",
    "            query_mask = padding_mask\n",
    "        )\n",
    "        normalized_attention = self.normalization_layer(mha_output + inputs)\n",
    "        dense_output = self.dense_proj(normalized_attention)\n",
    "        return self.normalization_layer2(normalized_attention + dense_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"key_dim\": self.key_dim\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = embed_dim // num_heads\n",
    "        self.self_attention_layer = layers.MultiHeadAttention(\n",
    "            key_dim=self.key_dim, num_heads=self.num_heads\n",
    "        )\n",
    "        self.cross_attention_layer = layers.MultiHeadAttention(\n",
    "            key_dim=self.key_dim, num_heads=self.num_heads\n",
    "        )\n",
    "        self.normalization1 = layers.LayerNormalization()\n",
    "        self.normalization2 = layers.LayerNormalization()\n",
    "        self.normalization3 = layers.LayerNormalization()\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(self.dense_dim, activation='relu'),\n",
    "            layers.Dense(self.embed_dim)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        self_attention_input, cross_attention_input = inputs\n",
    "        # causal_mask = self.get_causal_mask(ops.shape(inputs))\n",
    "        \n",
    "        if mask is not None:\n",
    "            self_attention_padding_mask, cross_attention_padding_mask = mask\n",
    "        else:\n",
    "            self_attention_padding_mask, cross_attention_padding_mask = None\n",
    "            \n",
    "        self_attention = self.self_attention_layer(\n",
    "            query = self_attention_input,\n",
    "            key = self_attention_input,\n",
    "            value = self_attention_input,\n",
    "            query_mask = self_attention_padding_mask,\n",
    "            use_causal_mask = True\n",
    "        )\n",
    "        normal_added_self_attention = self.normalization1(self_attention + self_attention_input)\n",
    "        \n",
    "        cross_attention = self.cross_attention_layer(\n",
    "            query = normal_added_self_attention,\n",
    "            key = cross_attention_input,\n",
    "            value = cross_attention_input,\n",
    "            query_mask = self_attention_padding_mask,\n",
    "            key_mask = cross_attention_padding_mask,\n",
    "        )\n",
    "        normal_added_cross_attention = self.normalization2(normal_added_self_attention + cross_attention)\n",
    "        decoder_output = self.dense_proj(normal_added_cross_attention)\n",
    "        return self.normalization3(normal_added_cross_attention + decoder_output)\n",
    "        \n",
    "    # def get_causal_mask(input_shape):\n",
    "    #     batch_size, sequence_len = input_shape[0], input_shape[1]\n",
    "    #     i = ops.arange(sequence_len)[:None]\n",
    "    #     j = ops.arange(sequence_len)\n",
    "    #     mask = ops.cast(i >= j, dtype='int32')\n",
    "    #     mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "    #     mult = ops.concatenate(\n",
    "    #         [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
    "    #         axis=0,\n",
    "    #     )\n",
    "    #     return ops.tile(mask, mult)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"key_dim\": self.key_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ef4cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'encoder_4' (of type Encoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/keras/src/layers/layer.py:1474: UserWarning: Layer 'decoder_4' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''cannot unpack non-iterable NoneType object''\n",
      "  warnings.warn(\n",
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'decoder_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/danieljoo/Code/ai_bootcamp/venv/lib/python3.9/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'decoder_4' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,502,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_8         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,502,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">119,688</span> │ position_embeddi… │\n",
       "│                     │                   │            │ not_equal_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">158,676</span> │ position_embeddi… │\n",
       "│                     │                   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,515,000</span> │ decoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │  \u001b[38;5;34m1,502,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_8         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │  \u001b[38;5;34m1,502,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_4 (\u001b[38;5;33mEncoder\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │    \u001b[38;5;34m119,688\u001b[0m │ position_embeddi… │\n",
       "│                     │                   │            │ not_equal_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_4 (\u001b[38;5;33mDecoder\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │    \u001b[38;5;34m158,676\u001b[0m │ position_embeddi… │\n",
       "│                     │                   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15000\u001b[0m) │  \u001b[38;5;34m1,515,000\u001b[0m │ decoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,797,364</span> (18.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,797,364\u001b[0m (18.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,797,364</span> (18.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,797,364\u001b[0m (18.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "embed_dim = 100\n",
    "dense_dim = 4 * embed_dim\n",
    "num_heads = 6\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(sequence_length,), dtype='int32', name=\"encoder_inputs\")\n",
    "x = PositionEmbedding(vocab_size=vocab_size, embed_dim=100, sequence_len=sequence_length)(encoder_inputs)\n",
    "encoder_output = Encoder(embed_dim=embed_dim, dense_dim = dense_dim, num_heads=num_heads)(x)\n",
    "\n",
    "decoder_self_attention_inputs = keras.Input(shape=(sequence_length,), dtype='int32',  name=\"decoder_inputs\")\n",
    "y = PositionEmbedding(vocab_size=vocab_size, embed_dim=100, sequence_len=sequence_length)(decoder_self_attention_inputs)\n",
    "decoder_output = Decoder(embed_dim=embed_dim, dense_dim = dense_dim, num_heads=num_heads)((y, encoder_output))\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(decoder_output)\n",
    "\n",
    "transformer = keras.Model(inputs=[encoder_inputs,decoder_self_attention_inputs], outputs=outputs)\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d9b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 237ms/step - accuracy: 0.0750 - loss: 5.5264 - val_accuracy: 0.1473 - val_loss: 3.2575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3378654c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1  # This should be at least 30 for convergence\n",
    "transformer.compile(\n",
    "    \"rmsprop\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb8f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: Tom and Mary lost their jobs due to budget cutbacks.\n",
      "output sentence: [start] y mary se [UNK] su [UNK] [UNK] [UNK] [end]\n",
      "input sentence: Tom needs money.\n",
      "output sentence: [start] dinero [end]\n",
      "input sentence: Stop wasting everyone's time.\n",
      "output sentence: [start] de [UNK] de tiempo [end]\n",
      "input sentence: It is too difficult a problem for me to solve.\n",
      "output sentence: [start] demasiado bien por un problema para mí [end]\n",
      "input sentence: This dictionary, of which the third volume is missing, cost me a hundred dollars.\n",
      "output sentence: [start] de que el mundo es el mundo puede [UNK] un día de la [UNK] [end]\n",
      "input sentence: I want you to grow up.\n",
      "output sentence: [start] que te [UNK] [end]\n",
      "input sentence: We're on strike because the company hasn't improved our wages.\n",
      "output sentence: [start] en el mundo se ha sido la [UNK] no se ha estado en nuestra [UNK] [end]\n",
      "input sentence: That box is bigger than this one.\n",
      "output sentence: [start] es más más que esto [end]\n",
      "input sentence: Learning a foreign language is difficult.\n",
      "output sentence: [start] una niña de [UNK] es difícil [end]\n",
      "input sentence: When was the last time you smoked a cigarette?\n",
      "output sentence: [start] la última vez que te ha sido un [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            {\n",
    "                \"encoder_inputs\": tokenized_input_sentence,\n",
    "                \"decoder_inputs\": tokenized_target_sentence,\n",
    "            }\n",
    "        )\n",
    "        sampled_token_index = ops.convert_to_numpy(\n",
    "            ops.argmax(predictions[0, i, :])\n",
    "        ).item(0)\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(10):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(f\"input sentence: {input_sentence}\\noutput sentence: {translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759f42a",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Woah, this might be the coolest project I did so far. For instance:\n",
    "\n",
    ">input sentence: It is too difficult a problem for me to solve.\\\n",
    "output sentence: [start] demasiado bien por un problema para mí [end]\n",
    "\n",
    "Ok, it's bad, but I'm honestly impressed by how the language is able to get this far with just random weights. Here, the output sentence word by word (which is not always the best translation mind you) translates to \"too much well for a problem for me\". The output is obviously nonsensicle with just one epoch of training, but you can tell that the machine read the input, had a very rough idea of the words, and maintains some structure in its own output for example by putting \"for\" and \"for a\" before a noun.\n",
    "\n",
    "Also, I had no clue how masks worked before, and now I find it especially sick how an simple attention mask of 1s with 0s above the diagonal can enforce causality. I'm starting to realize that the overall theme of neural networks is how stacking pieces of complexity via mathematical features contributes is analogous to enforcing rules on logic, which feels oddly metaphysical for such a rigid field. Apparently its also called 'inductive bias'.\n",
    "\n",
    "As far as improvement goes, I honestly am not totally sure if this works perfectly as expected since I can't really train it for more than a few epochs (one epoch takes 5 minutes). I believe the way I wrote it is close to optimal (pretty closely follows the example docs). If I had to debug it, I think I would appreciate explicitly passing around masks and remove mask ambiguity for the decoder block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
