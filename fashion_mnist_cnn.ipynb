{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e866f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e4e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06911e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f97e6af5",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this simple example, we'll use fashion MNIST, first with just dense mlp layers, then with including a custom convolutional layer. If it's interesting, I will bake in some optimizations for the convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0617825",
   "metadata": {},
   "source": [
    "### Simple MLP Model\n",
    "accuracy is around .86, and runs in less than 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ee9a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7184 - loss: 0.7936 - val_accuracy: 0.8527 - val_loss: 0.4077\n",
      "Epoch 2/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.4388 - val_accuracy: 0.8629 - val_loss: 0.3868\n",
      "Epoch 3/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8586 - loss: 0.3984 - val_accuracy: 0.8596 - val_loss: 0.3958\n",
      "Epoch 4/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8683 - loss: 0.3729 - val_accuracy: 0.8769 - val_loss: 0.3603\n",
      "Epoch 5/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8730 - loss: 0.3695 - val_accuracy: 0.8763 - val_loss: 0.3717\n",
      "Epoch 6/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8759 - loss: 0.3629 - val_accuracy: 0.8759 - val_loss: 0.3881\n",
      "Epoch 7/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8800 - loss: 0.3539 - val_accuracy: 0.8711 - val_loss: 0.4376\n",
      "Epoch 8/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8799 - loss: 0.3513 - val_accuracy: 0.8770 - val_loss: 0.3831\n",
      "Epoch 9/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8787 - loss: 0.3523 - val_accuracy: 0.8768 - val_loss: 0.4036\n",
      "Epoch 10/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8803 - loss: 0.3458 - val_accuracy: 0.8716 - val_loss: 0.4087\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 0.8681 - loss: 0.4208\n",
      "Test Loss: 0.43589258193969727\n",
      "Test Accuracy: 0.8662999868392944\n"
     ]
    }
   ],
   "source": [
    "simple_mlp_model = keras.Sequential([\n",
    "    keras.Input(shape=(28,28)),\n",
    "    keras.layers.Rescaling(1.0 / 255.0),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "simple_mlp_model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.RMSprop(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = simple_mlp_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.15\n",
    ")\n",
    "\n",
    "test_scores=simple_mlp_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Loss: {test_scores[0]}')\n",
    "print(f'Test Accuracy: {test_scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928f919",
   "metadata": {},
   "source": [
    "### Keras Convolution and MaxPool2D\n",
    "accuracy is around .90 and takes a few minutes to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03b7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.7165 - loss: 0.8453 - val_accuracy: 0.8679 - val_loss: 0.3759\n",
      "Epoch 2/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.3680 - val_accuracy: 0.8859 - val_loss: 0.3135\n",
      "Epoch 3/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8931 - loss: 0.3079 - val_accuracy: 0.9038 - val_loss: 0.2855\n",
      "Epoch 4/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9021 - loss: 0.2773 - val_accuracy: 0.8891 - val_loss: 0.3351\n",
      "Epoch 5/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9139 - loss: 0.2571 - val_accuracy: 0.8976 - val_loss: 0.2974\n",
      "Epoch 6/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9187 - loss: 0.2364 - val_accuracy: 0.9024 - val_loss: 0.2862\n",
      "Epoch 7/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9222 - loss: 0.2296 - val_accuracy: 0.9081 - val_loss: 0.2977\n",
      "Epoch 8/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9264 - loss: 0.2211 - val_accuracy: 0.9068 - val_loss: 0.3045\n",
      "Epoch 9/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9256 - loss: 0.2194 - val_accuracy: 0.9030 - val_loss: 0.3515\n",
      "Epoch 10/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9275 - loss: 0.2153 - val_accuracy: 0.9016 - val_loss: 0.3240\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8991 - loss: 0.3228\n",
      "Test Loss: 0.3272009789943695\n",
      "Test Accuracy: 0.901199996471405\n"
     ]
    }
   ],
   "source": [
    "keras_cnn_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Rescaling(1.0 / 255.0),\n",
    "\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(keras.activations.relu),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(keras.activations.relu),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "keras_cnn_model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.RMSprop(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = keras_cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.15\n",
    ")\n",
    "\n",
    "test_scores=keras_cnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Loss: {test_scores[0]}')\n",
    "print(f'Test Accuracy: {test_scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc5087",
   "metadata": {},
   "source": [
    "### My implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cb3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm fixing stride and padding. Bite me\n",
    "class MyConv2D(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size ,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.k_h, self.k_w = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.input_channels = input_shape[-1]\n",
    "        self.kernel_weights = self.add_weight(\n",
    "            shape=(self.k_h,self.k_w, self.input_channels, self.filters), #channels last convention in keras\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.filters,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    # nested for loops were a disaster so I'm doing the extra credit im2col method\n",
    "    def call(self, inputs):\n",
    "        batch_size = ops.shape(inputs)[0]\n",
    "        input_h, input_w = ops.shape(inputs)[1], ops.shape(inputs)[2]\n",
    "        k_h, k_w = self.k_h, self.k_w\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=inputs,\n",
    "            sizes=[1,k_h, k_w, 1],\n",
    "            strides=[1,1,1,1],\n",
    "            rates=[1,1,1,1],\n",
    "            padding='SAME'\n",
    "        )\n",
    "        patches_reshaped = ops.reshape(patches, (batch_size, -1, ops.shape(patches)[-1]))\n",
    "        patches_reshaped = ops.reshape(patches_reshaped, (-1, ops.shape(patches_reshaped)[-1]))\n",
    "        kernel_reshaped = ops.reshape(self.kernel_weights, (-1, self.filters))\n",
    "        output = ops.matmul(patches_reshaped, kernel_reshaped)\n",
    "        output = output + self.bias\n",
    "        out_h = input_h\n",
    "        out_w = input_w\n",
    "        final_output = ops.reshape(output, (batch_size, out_h, out_w, self.filters))\n",
    "        return final_output\n",
    "\n",
    "                \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\" : self.kernel_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MyBatchNormalization(keras.layers.Layer):\n",
    "    def __init__(self, momentum = 0.99, epsilon = 1e-3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        num_channels = input_shape[-1]\n",
    "        self.gamma = self.add_weight(\n",
    "            name='gamma',\n",
    "            shape=(num_channels,),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=(num_channels,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.moving_mean = self.add_weight(\n",
    "            name='moving_mean',\n",
    "            shape=(num_channels,),\n",
    "            initializer='zeros',\n",
    "            trainable=False\n",
    "        )\n",
    "        self.moving_variance = self.add_weight(\n",
    "            name='moving_variance',\n",
    "            shape=(num_channels,),\n",
    "            initializer='ones',\n",
    "            trainable=False\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            if len(inputs.shape) == 4:\n",
    "                axes = [0,1,2]\n",
    "            else:\n",
    "                axes = [0]\n",
    "            batch_mean = ops.mean(inputs, axis=axes)\n",
    "            batch_variance = ops.var(inputs, axis=axes)\n",
    "\n",
    "            self.moving_mean.assign(\n",
    "                self.moving_mean * self.momentum + batch_mean * (1 - self.momentum)\n",
    "            )\n",
    "            self.moving_variance.assign(\n",
    "                self.moving_variance * self.momentum + batch_variance * (1 - self.momentum)\n",
    "            )\n",
    "            \n",
    "            normalized_inputs = (inputs - batch_mean) / ops.sqrt(batch_variance + self.epsilon)\n",
    "\n",
    "        else:\n",
    "            normalized_inputs = (inputs - self.moving_mean) / ops.sqrt(self.moving_variance + self.epsilon)\n",
    "\n",
    "        return self.gamma * normalized_inputs + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23be6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.7244 - loss: 0.8281 - val_accuracy: 0.8780 - val_loss: 0.3421\n",
      "Epoch 2/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.8773 - loss: 0.3484 - val_accuracy: 0.8964 - val_loss: 0.3068\n",
      "Epoch 3/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.8929 - loss: 0.2995 - val_accuracy: 0.9074 - val_loss: 0.2614\n",
      "Epoch 4/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9062 - loss: 0.2645 - val_accuracy: 0.8849 - val_loss: 0.3418\n",
      "Epoch 5/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9126 - loss: 0.2503 - val_accuracy: 0.9061 - val_loss: 0.2677\n",
      "Epoch 6/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9154 - loss: 0.2433 - val_accuracy: 0.9102 - val_loss: 0.2797\n",
      "Epoch 7/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9207 - loss: 0.2381 - val_accuracy: 0.9100 - val_loss: 0.3318\n",
      "Epoch 8/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9223 - loss: 0.2299 - val_accuracy: 0.9074 - val_loss: 0.2900\n",
      "Epoch 9/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9242 - loss: 0.2285 - val_accuracy: 0.9033 - val_loss: 0.2928\n",
      "Epoch 10/10\n",
      "\u001b[1m797/797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9264 - loss: 0.2193 - val_accuracy: 0.9097 - val_loss: 0.3172\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9038 - loss: 0.3626\n",
      "Test Loss: 0.35706591606140137\n",
      "Test Accuracy: 0.90420001745224\n"
     ]
    }
   ],
   "source": [
    "keras_cnn_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Rescaling(1.0 / 255.0),\n",
    "\n",
    "    MyConv2D(filters=16, kernel_size=(3, 3)),\n",
    "    MyBatchNormalization(),\n",
    "    keras.layers.Activation(keras.activations.relu),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    MyConv2D(filters=32, kernel_size=(3, 3)),\n",
    "    MyBatchNormalization(),\n",
    "    keras.layers.Activation(keras.activations.relu),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "keras_cnn_model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.RMSprop(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = keras_cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.15\n",
    ")\n",
    "\n",
    "test_scores=keras_cnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Loss: {test_scores[0]}')\n",
    "print(f'Test Accuracy: {test_scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34007a",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "The overall takeaways on what the architecture does is relatively simple, but the implementation is suprisingly complex, especially if you want the model to not suck (using im2col for faster training and batch normalization). To be honest, my implementation is more so a hodgepodge of snippets and formulas online that I just pasted in, and it worked. To be honest, for the first time, I don't think I need to fully understand the mathematical details of the im2col trick.\n",
    "\n",
    "I find batch normalization to be a cool trick though. It's basically giving hyperparameters to the neural network to optimize like inputs. This is only possible due to the differentiability of the gamma switch, which we can't say about something like dropout (a binary switch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
